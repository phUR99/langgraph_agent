## Tool Calling 결과로 LLM 답변 생성하기
### 1. LLM이 웹 검색이 필요한지 판단
### 2. 필요하다면 검색을 수행
### 3. 최종적으로 응답을 생성
```python
from datetime import datetime
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableConfig, chain

today = datetime.today().strftime("%Y-%m-%d")

prompt = ChatPromptTemplate([
    ("system", f"You are a helpful AI assistant. Today's date is {today}."),
    ("human", "{user_input}"),
    '''
        LLM 체인은 대화 흐름을 메시지 단위로 주고받는 구조
        sys -> user -> tool -> ai res.의 list를 구성하는 구조.. 
        따라서 placeholder를 이용하여 이 자리에 툴 응답 or 대화 기록이 들어갈 자리를 구성해줌.
        messages는 반드시 list의 형태
    '''
    ("placeholder", "{messages}")
])

llm = ChatOllama(model="llama3.1:latest")
#web_search라는 tool을 등록해서 LLM이 필요할 경우 웹 검색을 요청할 수 있게 함.
llm_with_tools = llm.bind_tools(tools=[web_search])
#프롬프트 + LLM + 툴을 체인으로 연결
llm_chain = prompt | llm_with_tools

#체인 사용자 정의
'''
LangChain의 체인 객채로 등록되는 함수
LangChain 내부에서 runnable로 처리 가능. -> invoke(), stream(), batch() 등의 메서드 사용 가능

활용?
LangChain은 다양한 Runnable들을 연결해서 체인을 구성 -> prompt | model

RunnableConfig
LangChain에서 체인을 실행할 때 옵션이나 실행 설정을 넘기기 위한 컨테이너
-> invoke(), stream() 등에서 "이 체인을 어떻게 실행할까?"를 제어하는 설정값의 묶음.
-run_name: 실행에 이름을 붙임(trace)
-tags: 실행에 태그를 달 수 있음
-metadata: 실행에 메타데이터를 붙일 수 있음.
-callbacks: 콜백 함수들
-> LangChain의 실행 과정에 맞춰서 이벤트를 출력할 수 있음
        1. LLM의 응답을 실시간으로 출력
        2. 실행 로그를 수집
        3. 사용자 정의를 로깅
-recursion_limit: 체인의 내부 재귀 제한(루프 방지)
'''
@chain
def web_search_chain(user_input:str, config:RunnableConfig):
    input_ = {"user_input":user_input}
    #LLM을 한 번 호출해서 답변을 받음(tool 호출 가능)
    ai_msg = llm_chain.invoke(input_, config=config)
    print("ai_msg: \n", ai_msg)
    print("-"*100)
    #LLM이 요청한 tool_calls을 모아서 한번에 실행 -> web_search
    tool_msgs = web_search.batch(ai_msg.tool_calls, config=config)
    ''' 
        tool_calls 발생 조건
        1. LLM이 tool을 알고 있을 때(llm.bind_tools())
        2. LLM prompt에서 tool을 써야 할 이유가 있을 때.
        3. LLM이 tool을 사용하겠다 판단했을 때.

        판단 조건
        -> 실시간, 현재, 오늘, 요즘, 검색해줘 등의 키워드
        -> 모델이 툴 사용 학습 & 파인튜닝 완료
        -> 이전에 툴 응답이 없거나 부족하다고 판단
    '''
    print(tool_msgs)
    print("-"*100)
    # ** 딕셔너리 (키=밸류)언팩 * 리스트 언팩
    return   llm_chain.invoke({**input_, "messages": [ai_msg,*tool_msgs]}, config=config)

response = web_search_chain.invoke("오늘 모엣샹동 샴페인의 가격은 얼마인가요?")

pprint(response.content)

```
----------------------------------------------------------------------------------------------------
'오늘 모엣샹동 샴페인의 가격은 판매가격이 57,000원입니다. 원산지는 프랑스이며 알콜도수는 12%로 용량은 750ml입니다.'

## TOOL
랭체인은 사용자가 직접 도구를 정의하여 사용하는 방법을 사용
`@tool` 데코레이터를 사용하는 방법
-> 함수를 LangChain 도구로 변환하는 방법 <br>
-> 도구 함수 작성 가이드라인: 명확한 입출력의 정의, 단일 책임 원칙을 준수 <br>
-> 도구 설명 작성(description): LLM이 도구의 기능을 정확히 이해하고 사용하도록 작성 <br>

```python
from langchain_community.tools import TavilySearchResults
from langchain_core.tools import tool
from typing import List

@tool
def search_web(qeury:str) -> List[str]:
    #description 부분으로 llm이 도구의 기능을 정확히 이해하고 사용할 수 있다.
    """Searches the internet for info. that does not exist in the database or for the latest info."""
    tavily_search = TavilySearchResults(max_results=2)
    #docstring 부분을 잘 정의하는 것이 핵심.
    docs = tavily_search.invoke(query)
    #url의 경우 html 태그로 전달.
    formatted_docs = "\n---\n".join([
        f'<Document hreg="{doc["url"]}"/>\n{doc['content']}\n</Document>' for doc in docs
    ])
    if len(formatted_docs) > 0:
        return formatted_docs
    
    return "관련 정보를 찾을 수 없습니다."

```

### .env 깃헙에 잘못해서 올리다가 push에 error 발생
### 해결 방법
1. git reset을 이용해서 .env가 올라간 커밋 히스토리 찾아서 reset.
2. 다시 commit 이후 push